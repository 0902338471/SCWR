{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CinnamonAI-NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyQkCzBatnYp1lEYa83kZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0902338471/SCWR/blob/master/CinnamonAI_NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAIwCaYyq115",
        "colab_type": "text"
      },
      "source": [
        "**Hu-moment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iETFGFUQ5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuWJJbOqUfOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet:\n",
        "  def __init__(self,layers_dims):\n",
        "    self.params = {}\n",
        "    self.init_params(layers_dims)\n",
        "  def init_params(self,layer_dims):\n",
        "    ###initializing weight and bias for each layer using given layer_dims\n",
        "    self.num_hidden_layers = len(layer_dims)-1\n",
        "    for i in range(1,len(layer_dims)):\n",
        "      self.params[\"W\"+str(i)] = np.random.randn(layer_dims[i],layer_dims[i-1])\n",
        "      self.params[\"b\"+str(i)] = np.random.randn(layer_dims[i],1)\n",
        "  def unit_forward_propagating(self,previous_input,weight,bias):\n",
        "    ###calculate matrix operation and adding bias to unit input\n",
        "    return np.add(np.matmul(previous_input,weight),bias)\n",
        "  def softmax_function(self,a):\n",
        "    return a/(np.sum(a))\n",
        "  def forward_activate_output(self,activation,linear_input):\n",
        "    ###activating input by using activation function\n",
        "    if(activation=='sigmoid'):\n",
        "      return 1.0/(1+np.exp(-linear_input))\n",
        "    if(activation==\"softmax\"):\n",
        "      return np.apply_along_axis(self.softmax_function,1,linear_input)\n",
        "  def whole_forward_propagating(self,X):\n",
        "    ###whole forward propagating process\n",
        "    previous_input = copy.copy(X)\n",
        "    for layer in range(1,self.num_hidden_layers):\n",
        "      self.variables[\"Z\"+str(layer)] = unit_forward_propagating(previous_input,self.params[\"W\"+str(layer)],self.params[\"b\"+str(layer)])\n",
        "      if(layer == self.num_hidden_layers - 1):\n",
        "        ###softmax activation for last layer\n",
        "        self.variables[\"A\"+str(layer)] = forward_activate_output(activation=\"softmax\",self.variables[\"Z\"+str(layer)])\n",
        "      else:\n",
        "        self.variables[\"A\"+str(layer)] = forward_activate_output(activation=\"sigmoid\",self.variables[\"Z\"+str(layer)])\n",
        "      previous_input = copy.copy(self.variables[\"A\"+str(layer)])\n",
        "  def categorical_to_one_hot(self,Y):\n",
        "    num_class = np.max(Y)+1\n",
        "    return np.eye(num_class)[Y]\n",
        "  def compute_each_instances(self,unit_Y,unit_last_layer):\n",
        "    ### unit_Y and unit_last_layer both have same shape of (n_class,1) (Y must be converted to one hot vector)\n",
        "    return np.dot(unit_Y,unit_last_layer)\n",
        "  def calculate_error_last_layer(self,)\n",
        "  def cost_function(self,Y,last_layer):\n",
        "    ###total cost over all examples\n",
        "    Y_one_hot = self.categorical_to_one_hot(Y)\n",
        "    cost = 0.0\n",
        "    for index_example,example in enumerate(last_layer):\n",
        "      cost += compute_each_instance(Y_one_hot[index_example],example)\n",
        "    cost = -cost/m\n",
        "    return cost\n",
        "  def unit_backward_propagating(self,next_input):\n",
        "  def backward_activate_output(self,weight,derive_linear_input):\n",
        "    ###calculating derivative of J with respect to activation unit \n",
        "      return np.matmul(np.transpose(weight),derive_linear_input)\n",
        "  def \n",
        "  def whole_backward_propagating(self,X):\n",
        "    ###whole backward propagating process\n",
        "  def fit_model(X):\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHzmwRPuLh07",
        "colab_type": "text"
      },
      "source": [
        "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6XQIGK3pSY2",
        "colab_type": "code",
        "outputId": "cac5d695-e670-4264-cd94-d217bc83e96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.dot([1,2,3],[1,2,3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ1nqIhcazPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_to_one_hot(Y):\n",
        "  num_class = np.max(Y)+1\n",
        "  return np.eye(num_class)[Y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0mBS68NobZK",
        "colab_type": "code",
        "outputId": "badae7b9-03d7-4106-989d-cc38211c7d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "categorical_to_one_hot([1,2,3,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xgE0f-c-kI",
        "colab_type": "code",
        "outputId": "b57d7de8-a779-4654-b1dd-64cdcb7a711d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "-c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-24, -15],\n",
              "       [-54, -36]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFLwpZNvbZIb",
        "colab_type": "code",
        "outputId": "7c2e41b6-3dfc-451e-c2e8-cee07eff3e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "a=[[1,2,3],[4,5,6]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[25, 16],\n",
              "       [56, 38]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYmlyKG8mGqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_function(a):\n",
        "  return a/(np.sum(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUJgvLFamB-g",
        "colab_type": "code",
        "outputId": "d96848ee-fcd4-47e1-df4f-1058f08e3842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "np.apply_along_axis(my_function,1,a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16666667, 0.33333333, 0.5       ],\n",
              "       [0.26666667, 0.33333333, 0.4       ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}